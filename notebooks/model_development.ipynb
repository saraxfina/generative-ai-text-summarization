{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816d50dc-80de-47e3-9f42-a5270850ad56",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea80a1-57d1-45ed-a980-bc9db8305a8e",
   "metadata": {},
   "source": [
    "Objective: Develop a custom transformer model using Keras framework and then enhance the project by retraining a pre-trained model for comparision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e461f34-1474-4b48-9e76-3643c2cda081",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- Modular Code: Keep your code modular to facilitate easy switching between the custom model and the BART model for different experiments.\n",
    "- Documentation: Document each step in your model development process, including parameter settings and the rationale behind chosen architectures.\n",
    "- Version Control: Commit all changes, especially new scripts and configurations, to GitHub to maintain a robust version history.\n",
    "- Continuous Monitoring: Regularly monitor training progress using TensorBoard integrated with Keras to visualize performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa51f13-b0c1-4f97-bc2f-15aaab9cad9d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7907552d-187c-4381-a871-d20db0b6fe04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0d263d2-f2fb-4cad-8e48-7a76bbe049f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU enabled \n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499a78c-003e-4e96-bc47-429365b5dffc",
   "metadata": {},
   "source": [
    "### Develop Custom Transformer Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82760da2-bd41-4446-827b-97f1a83ccbda",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "603ac1c0-180e-4fd1-9771-c38e73c8604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.load('../data/cnn_dailymail.npz', allow_pickle=True)\n",
    "#data = np.load('../data/news_api_data.npz', allow_pickle=True)\n",
    "#data = np.load('../data/cnn_dailymail_small.npz', allow_pickle=True)\n",
    "data = np.load('../data/news_api_data_small.npz', allow_pickle=True)\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1e2d1787-53fb-4c66-bbfb-e9158748daf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17180"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test, X_val))\n",
    "y = np.concatenate((y_train, y_test, y_val))\n",
    "max_len = len(max(X, key=len))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "092860d6-3eb0-4390-bcdb-7eb44ee1ab77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_len)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "y_train = tokenizer.texts_to_sequences(y_train)\n",
    "y_test = tokenizer.texts_to_sequences(y_test)\n",
    "y_val = tokenizer.texts_to_sequences(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0deb537-7ea4-430e-91f1-2bb4822a201b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize Data by padding sequences\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "X_val = pad_sequences(X_val, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7c1dc-d31c-41c1-9893-cf96a41b3c8d",
   "metadata": {},
   "source": [
    "### Select and Retrain Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27988f93-08a8-42b5-8895-0f81da1adf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Transformer Block\n",
    "def transformer_block(x):\n",
    "\n",
    "    # MultiHead Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)\n",
    "    attn_output = Dropout(.2)(attn_output)\n",
    "    out1 = LayerNormalization()(x + attn_output)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    ffn_output = Dense(2048, activation='relu')(out1)\n",
    "    ffn_output = Dropout(.2)(ffn_output)\n",
    "    ffn_output = Dense(64, activation='relu')(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3bea3be6-e313-4380-8d15-9a154880119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len):\n",
    "\n",
    "  vocab_size = 10000\n",
    " \n",
    "  inputs = Input(shape=(max_len,))\n",
    "  embedding_layer = Embedding(vocab_size, 64)(inputs)\n",
    "\n",
    "  x = transformer_block(embedding_layer)\n",
    "  x = GlobalAveragePooling1D()(x)\n",
    "  x = Dropout(0.1)(x) # add droput layer\n",
    "  outputs = Dense(2, activation=\"softmax\")(x) # dense layer\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=outputs) # define inputs and outputs\n",
    "  model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) # define params\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00db2fc9-8529-4f0f-9227-282707031723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de96a5d9-2c8c-4bb3-b7a2-417f9d6d6a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[  0   0   0 ...   1 199 200]\n [  0   0   0 ...  93  11 209]\n [  0   0   0 ...  54 111 113]\n [  0   0   0 ...  15   4 354]\n [  0   0   0 ... 368  16 369]\n [  0   0   0 ...  32 378   9]] (of type <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.10/site-packages/keras/src/trainers/data_adapters/__init__.py:113\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x=[[  0   0   0 ...   1 199 200]\n [  0   0   0 ...  93  11 209]\n [  0   0   0 ...  54 111 113]\n [  0   0   0 ...  15   4 354]\n [  0   0   0 ... 368  16 369]\n [  0   0   0 ...  32 378   9]] (of type <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=32, epochs=2, validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416774a-e2df-467e-acb4-af022163cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2877a7f-2f69-41e0-8107-df99e5d53cb1",
   "metadata": {},
   "source": [
    "### Setup MLflow for Experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed0005-06c0-418f-9e1c-c692d788cfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6feb170-5115-4665-beaf-a2fa82390fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebeecb3e-a059-4ee4-ab4b-b5c175694fda",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training and Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ede4d-9cde-4ee0-acac-775657d5919f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
